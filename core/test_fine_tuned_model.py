# core/test_fine_tuned_model_fixed.py
from model_loader import load_peft_model_simple
import torch

def test_rule_generation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer = load_peft_model_simple(
        "codellama/CodeLlama-7b-hf",
        "outputs/codeLlama-7b-semgrep-lora"
    )
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∞–≤–∏–ª–∞...")
    
    # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è CodeLlama
    test_prompt = """[INST] <<SYS>>
    –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –∫–æ–¥–∞. –°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è SQL-–∏–Ω—ä–µ–∫—Ü–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é —Å—Ç—Ä–æ–∫ –≤ Python.
    <</SYS>>

    –°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–π —É—è–∑–≤–∏–º–æ—Å—Ç–∏:

    –û–ø–∏—Å–∞–Ω–∏–µ: SQL-–∏–Ω—ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é —Å—Ç—Ä–æ–∫
    –ü—Ä–∏–º–µ—Ä —É—è–∑–≤–∏–º–æ–≥–æ –∫–æ–¥–∞:
    ```python
    query = "SELECT * FROM users WHERE id = " + user_input```
    –°–æ–∑–¥–∞–π —Ç–æ—á–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ Semgrep –≤ —Ñ–æ—Ä–º–∞—Ç–µ YAML. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û YAML-–∫–æ–¥ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. [/INST]"""

    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª–∞...")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    inputs = tokenizer(test_prompt, return_tensors="pt", max_length=2048, truncation=True)
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ inputs –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
    device = model.device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
            no_repeat_ngram_size=3
        )
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å (–∏—Å–∫–ª—é—á–∞—è –ø—Ä–æ–º–ø—Ç)
    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    print("=" * 60)
    print("üéØ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–û–ï –ü–†–ê–í–ò–õ–û:")
    print("=" * 60)
    print(response)
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ YAML –≤ –æ—Ç–≤–µ—Ç–µ
    if "rules:" in response or "id:" in response:
        print("‚úÖ –ü—Ä–∞–≤–∏–ª–æ —Å–æ–¥–µ—Ä–∂–∏—Ç YAML-—Å—Ç—Ä—É–∫—Ç—É—Ä—É")
    else:
        print("‚ùå YAML-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
    
    return response

if __name__ == "__main__":
    test_rule_generation()