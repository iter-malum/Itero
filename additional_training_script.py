import json
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, PeftModel
from trl import SFTTrainer
import datasets
from datetime import datetime
import os

def load_and_prepare_data(train_path, val_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    
    with open(train_path, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    with open(val_path, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_data)} –æ–±—É—á–∞—é—â–∏—Ö –∏ {len(val_data)} –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    def format_for_training(examples):
        formatted_texts = []
        for i in range(len(examples)):
            instruction = examples[i]["instruction"]
            input_text = examples[i]["input"]
            output = examples[i]["output"]
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ —Å—Ç–∏–ª–µ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
            formatted = f"""<s>[INST] {instruction}

{input_text} [/INST] {output}</s>"""
            formatted_texts.append(formatted)
        
        return {"text": formatted_texts}
    
    train_dataset = datasets.Dataset.from_list(train_data)
    val_dataset = datasets.Dataset.from_list(val_data)
    
    train_dataset = train_dataset.map(format_for_training, batched=True)
    val_dataset = val_dataset.map(format_for_training, batched=True)
    
    return train_dataset, val_dataset

def setup_model_and_tokenizer(model_name="codellama/CodeLlama-7B-Instruct-hf"):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 5070 (12GB)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,  # –î–≤–æ–π–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        use_cache=False,  # –í–∞–∂–Ω–æ –¥–ª—è gradient checkpointing
    )
    
    return model, tokenizer

def setup_lora_config():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ Semgrep"""
    
    return LoraConfig(
        r=16,  # Rank - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
        lora_alpha=32,
        target_modules=[
            "q_proj", "v_proj", "k_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"  # –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è MLP
        ],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–Ω–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã YAML
        modules_to_save=None,
    )

def main():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    MODEL_NAME = "codellama/CodeLlama-7B-Instruct-hf"
    TRAIN_DATA_PATH = "semgrep_prepared_data/train_data.json"
    VAL_DATA_PATH = "semgrep_prepared_data/val_data.json"
    OUTPUT_DIR = f"semgrep-finetuned-{datetime.now().strftime('%Y%m%d-%H%M')}"
    
    print("üöÄ –ù–∞—á–∞–ª–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª Semgrep...")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_dataset, val_dataset = load_and_prepare_data(TRAIN_DATA_PATH, VAL_DATA_PATH)
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)
    
    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    print("üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
    lora_config = setup_lora_config()
    model = get_peft_model(model, lora_config)
    
    # 4. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ YAML
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á–µ–π –¥–ª—è RTX 5070
        per_device_train_batch_size=2,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è 12GB
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 8
        dataloader_pin_memory=False,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        num_train_epochs=4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞
        learning_rate=1.5e-4,  # –ù–µ–º–Ω–æ–≥–æ –Ω–∏–∂–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        warmup_steps=100,
        optim="paged_adamw_8bit",
        
        # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        weight_decay=0.01,
        max_grad_norm=0.3,
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=50,
        save_steps=500,
        evaluation_strategy="steps",
        eval_steps=200,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # FP16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        fp16=True,
        
        # Gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        gradient_checkpointing=True,
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        report_to=["tensorboard"],
        run_name=f"semgrep-finetune-{datetime.now().strftime('%Y%m%d-%H%M')}",
    )
    
    # 5. –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ Semgrep
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        dataset_text_field="text",
        tokenizer=tokenizer,
        max_seq_length=3072,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª Semgrep
        packing=False,  # –ù–µ —É–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        data_collator=DataCollatorForSeq2Seq(
            tokenizer,
            pad_to_multiple_of=8,
            return_tensors="pt",
            padding=True,
        ),
    )
    
    # 6. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    train_result = trainer.train()
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA
    trainer.model.save_pretrained(f"{OUTPUT_DIR}/lora-adapters")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/lora-adapters")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    with open(f"{OUTPUT_DIR}/training_metrics.json", "w") as f:
        json.dump(train_result.metrics, f, indent=2)
    
    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}")
    
    return trainer

def test_trained_model(model_path, tokenizer_path):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    base_model = AutoModelForCausalLM.from_pretrained(
        "codellama/CodeLlama-7B-Instruct-hf",
        device_map="auto",
        torch_dtype=torch.float16,
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    model = PeftModel.from_pretrained(base_model, model_path)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    test_prompts = [
        """–ù–∞–ø–∏—à–∏ –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è SQL injection –≤ Python""",
        
        """–°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –ø–æ–∏—Å–∫–∞ hardcoded API keys –≤ JavaScript""",
        
        """–î–æ—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è XSS –≤ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∞ Java"""
    ]
    
    for i, prompt in enumerate(test_prompts):
        print(f"\nüìù –¢–µ—Å—Ç {i+1}: {prompt}")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n{response}")

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    trainer = main()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    # test_trained_model("semgrep-finetuned/lora-adapters", "semgrep-finetuned/lora-adapters")