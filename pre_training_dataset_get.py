import json
import os
import glob
import random
from pathlib import Path
import yaml
from collections import defaultdict
from datetime import datetime

def safe_instruction_format(instruction_template, description):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    """
    try:
        # –ï—Å–ª–∏ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ —É–∂–µ –µ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
        if '{' in instruction_template and '}' in instruction_template:
            return instruction_template.format(desc=description)
        else:
            return instruction_template.replace('{desc}', description)
    except:
        # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
        return description

def enhance_instruction_variety(original_instruction, rule_type):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–æ–±—â–µ–Ω–∏—è
    """
    variations = {
        'create': [
            "–ù–∞–ø–∏—à–∏ –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è: {desc}",
            "–°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {desc}",
            "–†–∞–∑—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è: {desc}",
            "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –ø–æ–∏—Å–∫–∞: {desc}",
            "–°–æ—Å—Ç–∞–≤—å –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è: {desc}",
            "–ù–∞–ø–∏—à–∏ Semgrep –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {desc}",
            "–°–æ–∑–¥–∞–π Semgrep –ø—Ä–∞–≤–∏–ª–æ –¥–ª—è: {desc}"
        ],
        'modify': [
            "–î–æ—Ä–∞–±–æ—Ç–∞–π —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ Semgrep: {desc}",
            "–£–ª—É—á—à–∏ –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è: {desc}", 
            "–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {desc}",
            "–û–±–Ω–æ–≤–∏ –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è: {desc}"
        ]
    }
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    task_type = 'modify' if any(word in original_instruction.lower() for word in 
                               ['–¥–æ—Ä–∞–±–æ—Ç–∞–π', '–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π', '–æ–±–Ω–æ–≤–∏', '—É–ª—É—á—à–∏']) else 'create'
    
    # –ë–µ—Ä–µ–º 2 —Å–ª—É—á–∞–π–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ + –æ—Ä–∏–≥–∏–Ω–∞–ª
    selected_variations = random.sample(variations[task_type], 2) + [original_instruction]
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    formatted_instructions = []
    for variation in selected_variations:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        clean_description = original_instruction
        for prefix in ["–°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è ", "–ù–∞–ø–∏—à–∏ –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è "]:
            if original_instruction.startswith(prefix):
                clean_description = original_instruction[len(prefix):]
                break
        
        formatted_instruction = safe_instruction_format(variation, clean_description)
        formatted_instructions.append(formatted_instruction)
    
    return formatted_instructions

def validate_semgrep_yaml(yaml_text):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ YAML —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    """
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å YAML
        data = yaml.safe_load(yaml_text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if not data or 'rules' not in data:
            return False
            
        rule = data['rules'][0] if isinstance(data['rules'], list) and len(data['rules']) > 0 else data.get('rules', {})
        required_fields = ['id', 'message', 'languages', 'severity']
        
        return all(field in rule for field in required_fields)
        
    except Exception as e:
        print(f"    YAML validation error: {str(e)}")
        return False

def safe_get_examples(examples_list, max_examples=3):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
    """
    if not examples_list:
        return []
    
    safe_examples = []
    for example in examples_list[:max_examples]:
        if example and len(str(example)) < 1000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            safe_examples.append(str(example))
    
    return safe_examples

def create_learning_example(data, instruction_variation):
    """
    –°–æ–∑–¥–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    """
    try:
        code_context = data.get("code_context", {})
        metadata = data.get("metadata", {})
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞
        dangerous_examples = safe_get_examples(code_context.get("dangerous_examples", []))
        safe_examples = safe_get_examples(code_context.get("safe_examples", []))
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        input_context = f"""–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –Ø–∑—ã–∫: {data.get("language", metadata.get("language", ""))}
- –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {metadata.get("mode", "")}
- CWE: {data.get("CWE", metadata.get("cwe", ""))}
- –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {metadata.get("severity", "")}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–¥–∞:
{code_context.get("full_code", "")[:2000]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

–û–ø–∞—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:
{chr(10).join(f"‚Ä¢ {example}" for example in dangerous_examples)}

–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:  
{chr(10).join(f"‚Ä¢ {example}" for example in safe_examples)}"""

        # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        annotation_legend = code_context.get("annotation_legend", {})
        if annotation_legend:
            input_context += f"\n\n–õ–µ–≥–µ–Ω–¥–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:\n{chr(10).join(f'- {key}: {value}' for key, value in annotation_legend.items())}"
    
        # –§–æ—Ä–º–∏—Ä—É–µ–º output —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º
        reasoning = data.get("reasoning", "")
        output = f"""{data["output"]}

# –õ–æ–≥–∏–∫–∞ –ø—Ä–∞–≤–∏–ª–∞:
{reasoning}

# –ö–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:
- ID: {metadata.get("rule_id", "")}
- –Ø–∑—ã–∫: {metadata.get("language", "")} 
- –¢–∏–ø: {metadata.get("rule_type", "")}
- –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {metadata.get("severity", "")}"""
    
        return {
            "instruction": instruction_variation,
            "input": input_context,
            "output": output,
            "metadata": {
                "rule_id": metadata.get("rule_id", ""),
                "language": metadata.get("language", ""),
                "severity": metadata.get("severity", ""),
                "rule_type": metadata.get("rule_type", ""),
                "mode": metadata.get("mode", ""),
                "is_validation": False
            }
        }
    except Exception as e:
        print(f"    Error creating learning example: {str(e)}")
        return None

def transform_semgrep_data_enhanced(input_dir, output_file):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    """
    all_examples = []
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã
    json_files = glob.glob(os.path.join(input_dir, "**", "*.json"), recursive=True)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ JSON —Ñ–∞–π–ª–æ–≤: {len(json_files)}")
    
    language_stats = {}
    error_files = []
    
    for i, file_path in enumerate(json_files):
        if i % 100 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(json_files)} —Ñ–∞–π–ª–æ–≤...")
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ
            if not validate_semgrep_yaml(data.get("output", "")):
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ: {file_path}")
                error_files.append(f"Invalid YAML: {file_path}")
                continue
            
            language = data.get("language", data.get("metadata", {}).get("language", "unknown"))
            language_stats[language] = language_stats.get(language, 0) + 1
            
            # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
            instruction_variations = enhance_instruction_variety(
                data.get("instruction", ""), 
                data.get("metadata", {}).get("rule_type", "create")
            )
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞—Ä–∏–∞—Ü–∏–∏
            for instruction_var in instruction_variations:
                example = create_learning_example(data, instruction_var)
                if example is not None:
                    all_examples.append(example)
                
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {str(e)}"
            print(error_msg)
            error_files.append(error_msg)
            continue
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º:")
    for lang, count in sorted(language_stats.items()):
        print(f"   {lang}: {count} —Ñ–∞–π–ª–æ–≤")
    
    print(f"üéØ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {len(all_examples)}")
    
    if error_files:
        print(f"üö® –§–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {len(error_files)}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –æ—à–∏–±–æ–∫
        with open(output_file.replace('.json', '_errors.log'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(error_files))
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_examples, f, ensure_ascii=False, indent=2)
    
    return all_examples

def create_stratified_split(all_examples, val_ratio=0.1):
    """
    –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º –∏ —Ç–∏–ø–∞–º –ø—Ä–∞–≤–∏–ª
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø–æ —è–∑—ã–∫—É –∏ —Ç–∏–ø—É –ø—Ä–∞–≤–∏–ª–∞
    groups = defaultdict(list)
    
    for example in all_examples:
        key = (example["metadata"]["language"], example["metadata"].get("rule_type", "audit"))
        groups[key].append(example)
    
    train_data = []
    val_data = []
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã —Ä–∞–∑–¥–µ–ª—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    for group_key, examples in groups.items():
        random.shuffle(examples)
        
        split_index = int(len(examples) * (1 - val_ratio))
        train_data.extend(examples[:split_index])
        val_data.extend(examples[split_index:])
        
        # –ü–æ–º–µ—á–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        for example in val_data:
            example["metadata"]["is_validation"] = True
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã
    random.shuffle(train_data)
    random.shuffle(val_data)
    
    return train_data, val_data

def analyze_final_datasets(train_data, val_data):
    """
    –ê–Ω–∞–ª–∏–∑ –∏—Ç–æ–≥–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    print("\nüìà –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–û–í:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    total = len(train_data) + len(val_data)
    if total > 0:
        print(f"   –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {len(train_data)/total*100:.1f}% / {len(val_data)/total*100:.1f}%")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
    train_langs = defaultdict(int)
    val_langs = defaultdict(int)
    
    for example in train_data:
        train_langs[example["metadata"]["language"]] += 1
    for example in val_data:
        val_langs[example["metadata"]["language"]] += 1
    
    print("\nüåç –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º:")
    all_langs = set(train_langs.keys()) | set(val_langs.keys())
    for lang in sorted(all_langs):
        train_count = train_langs.get(lang, 0)
        val_count = val_langs.get(lang, 0)
        total = train_count + val_count
        if total > 0:
            print(f"   {lang}: {train_count} train, {val_count} val ({val_count/total*100:.1f}% val)")

def clean_duplicate_prefixes(instructions):
    """
    –û—á–∏—â–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
    """
    cleaned = []
    for instruction in instructions:
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ "–†–∞–∑—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è: –°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è"
        if "–°–æ–∑–¥–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è" in instruction and "–†–∞–∑—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è" in instruction:
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É —á–∞—Å—Ç—å
            instruction = instruction.split("–†–∞–∑—Ä–∞–±–æ—Ç–∞–π –ø—Ä–∞–≤–∏–ª–æ Semgrep –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è: ")[-1]
        cleaned.append(instruction)
    return cleaned

if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    INPUT_DIRECTORY = "semgrep_dataset_train"
    OUTPUT_DIR = "semgrep_prepared_data"
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    all_examples = transform_semgrep_data_enhanced(
        INPUT_DIRECTORY, 
        os.path.join(OUTPUT_DIR, "semgrep_full_dataset.json")
    )
    
    # 2. –û—á–∏—â–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
    print("\nüßπ –û—á–∏—Å—Ç–∫–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π...")
    for example in all_examples:
        example["instruction"] = clean_duplicate_prefixes([example["instruction"]])[0]
    
    # 3. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation
    print("\n‚úÇÔ∏è  –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation...")
    train_data, val_data = create_stratified_split(all_examples, val_ratio=0.1)
    
    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(os.path.join(OUTPUT_DIR, "train_data.json"), 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(os.path.join(OUTPUT_DIR, "val_data.json"), 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    analyze_final_datasets(train_data, val_data)
    
    # 6. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    config = {
        "dataset_info": {
            "total_examples": len(all_examples),
            "train_examples": len(train_data),
            "val_examples": len(val_data),
            "languages": list(set(ex["metadata"]["language"] for ex in all_examples)),
            "created_at": str(datetime.now())
        },
        "training_notes": "–°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —è–∑—ã–∫–∞–º –∏ —Ç–∏–ø–∞–º –ø—Ä–∞–≤–∏–ª"
    }
    
    with open(os.path.join(OUTPUT_DIR, "dataset_config.json"), 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {OUTPUT_DIR}")